{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import and Pre-Processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_raw = pd.read_csv('./data/train_notypes', sep='\\t', header=None, names=['word', 'label'])\n",
    "train_vocab = set(train_words_raw.word.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import namedtuple\n",
    "class RegexFeatures(object):\n",
    "    PATTERNS = {\n",
    "        \"isInitCapitalWord\": re.compile(r'^[A-Z][a-z]+'),\n",
    "        \"isAllCapitalWord\": re.compile(r'^[A-Z][A-Z]+$'),\n",
    "        \"isAllSmallCase\": re.compile(r'^[a-z]+$'),\n",
    "        \"isWord\": re.compile(r'^[a-zA-Z][a-zA-Z]+$'),\n",
    "        \"isAlphaNumeric\": re.compile(r'^\\p{Alnum}+$'),\n",
    "        \"isSingleCapLetter\": re.compile(r'^[A-Z]$'),\n",
    "        \"containsDashes\": re.compile(r'.*--.*'),\n",
    "        \"containsDash\": re.compile(r'.*\\-.*'),\n",
    "        \"singlePunctuation\": re.compile(r'^\\p{Punct}$'),\n",
    "        \"repeatedPunctuation\": re.compile(r'^[\\.\\,!\\?\"\\':;_\\-]{2,}$'),\n",
    "        \"singleDot\": re.compile(r'[.]'),\n",
    "        \"singleComma\": re.compile(r'[,]'),\n",
    "        \"singleQuote\": re.compile(r'[\\']'),\n",
    "        \"isSpecialCharacter\": re.compile(r'^[#;:\\-/<>\\'\\\"()&]$'),\n",
    "        \"fourDigits\": re.compile(r'^\\d\\d\\d\\d$'),\n",
    "        \"isDigits\": re.compile(r'^\\d+$'),\n",
    "        \"isNumber\": re.compile(r'^((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
    "        \"containsDigit\": re.compile(r'.*\\d+.*'),\n",
    "        \"endsWithDot\": re.compile(r'\\p{Alnum}+\\.$'),\n",
    "        \"isURL\": re.compile(r'^http[s]?://'),\n",
    "        \"isMention\": re.compile(r'^(RT)?@[\\p{Alnum}_]+$'),\n",
    "        \"isHashtag\": re.compile(r'^#\\p{Alnum}+$'),\n",
    "        \"isMoney\": re.compile(r'^\\$((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
    "    }\n",
    "    def __init__(self):\n",
    "        print(\"Initialized RegexFeature\")\n",
    "    @staticmethod\n",
    "    def process(word):\n",
    "        features = dict()\n",
    "        for k, p in RegexFeatures.PATTERNS.iteritems():\n",
    "            if p.match(word):\n",
    "                features[k] = True\n",
    "        return features\n",
    "    \n",
    "    \n",
    "def classification_report_to_df(report):\n",
    "    report_list = []\n",
    "    for i, line in enumerate(report.split(\"\\n\")):\n",
    "        if i == 0:\n",
    "            report_list.append([\"class\", \"precision\", \"recall\", \"f1-score\", \"support\"])\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if line.startswith(\"avg\"):\n",
    "                    line = line.replace(\"avg / total\", \"avg/total\")\n",
    "                line = re.split(r'\\s+', line)\n",
    "                report_list.append(tuple(line))\n",
    "    return pd.DataFrame(report_list[1:], columns=report_list[0])\n",
    "\n",
    "\n",
    "DATA_DIR=\"data/data/\"\n",
    "CLEANED_DIR=\"data/cleaned/\"\n",
    "\n",
    "Tag = namedtuple(\"Tag\", [\"token\", \"tag\"])\n",
    "\n",
    "def load_sequences(filename, sep=\"\\t\", notypes=False, test_data=False):\n",
    "    sequences = []\n",
    "    with open(filename) as fp:\n",
    "        seq = []\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line = line.split(sep)\n",
    "                if test_data:\n",
    "                    assert len(line) == 1\n",
    "                    line.append(\"?\")\n",
    "                if notypes:\n",
    "                    line[1] = line[1][0]\n",
    "                seq.append(Tag(*line))\n",
    "            else:\n",
    "                sequences.append(seq)\n",
    "                seq = []\n",
    "        if seq:\n",
    "            sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def load_vocab(filename):\n",
    "    vocab = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            vocab.add(line)\n",
    "    return vocab      \n",
    "\n",
    "    \n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "        \n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr)) \n",
    "        \n",
    "        \n",
    "def plot_cm(y_test, y_pred, labels=[], axis=1):\n",
    "    labels_s = dict((k,i) for i,k in enumerate(labels))\n",
    "    cm = np.zeros((len(labels), len(labels)))\n",
    "    for i,j in zip(sum(y_test, []), sum(y_pred, [])):\n",
    "        i = labels_s[i]\n",
    "        j = labels_s[j]\n",
    "        cm[i,j] += 1\n",
    "    with plt.rc_context(rc={'xtick.labelsize': 12, 'ytick.labelsize': 12,\n",
    "                       'figure.figsize': (16,14)}):\n",
    "        sns.heatmap(cm * 100/ cm.sum(axis=axis, keepdims=True),\n",
    "                    #cmap=sns.cubehelix_palette(n_colors=100, rot=-.4, as_cmap=True),\n",
    "                    cmap=\"Greys\",\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "        plt.ylabel(\"True labels\")\n",
    "        plt.xlabel(\"Predicted labels\")\n",
    "        title = \"Precision Plot\"\n",
    "        if axis== 0:\n",
    "            title = \"Recall Plot\"\n",
    "        plt.title(title)\n",
    "    print(cm.shape)\n",
    "    return cm\n",
    "\n",
    "\n",
    "def print_sequences(sequences, predictions, filename, test_data=False, notypes=False):\n",
    "    with open(filename, \"wb+\") as fp:\n",
    "        for seq, pred in zip(sequences, predictions):\n",
    "            for t, p in zip(seq, pred):\n",
    "                token, tag = t\n",
    "                if tag[0] == \"U\":\n",
    "                    tag = \"B%s\" % tag[1:]\n",
    "                if tag[0] == \"E\":\n",
    "                    tag = \"I%s\" % tag[1:]\n",
    "                if p[0] == \"U\":\n",
    "                    p = \"B%s\" % p[1:]\n",
    "                if p[0] == \"E\":\n",
    "                    p = \"I%s\" % p[1:]\n",
    "                if notypes:\n",
    "                    tag = tag[0]\n",
    "                    p = p[0]\n",
    "                if test_data:\n",
    "                    line = \"\\t\".join((token, p))\n",
    "                else:\n",
    "                    line = \"\\t\".join((token, tag, p))\n",
    "                print >> fp, line\n",
    "            print >> fp, \"\"\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = load_sequences(\"data/train\", sep=\"\\t\", notypes=False)\n",
    "dev_sequences = load_sequences(\"data/dev\", sep=\"\\t\", notypes=False)\n",
    "test_sequences = load_sequences(\"data/test\", sep=\"\\t\", notypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [[t[0] for t in seq] for seq in (train_sequences+dev_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the',\n",
       "   'wall',\n",
       "   'street',\n",
       "   'journal',\n",
       "   'reported',\n",
       "   'today',\n",
       "   'that',\n",
       "   'apple',\n",
       "   'corporation',\n",
       "   'made',\n",
       "   'money'],\n",
       "  ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']),\n",
       " (['georgia', 'tech', 'is', 'a', 'university', 'in', 'georgia'],\n",
       "  ['B', 'I', 'O', 'O', 'O', 'O', 'B'])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = [ (\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ") ]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    input_data = list()\n",
    "    sentence_word_list = list()\n",
    "    sentence_word_tag_list = list()\n",
    "    for line in data:\n",
    "        splitted_line = line.split(\"\\t\")\n",
    "\n",
    "        if splitted_line[0] == \"\\n\":\n",
    "            one_sentence_tuple = (sentence_word_list, sentence_word_tag_list)\n",
    "            input_data.append(one_sentence_tuple)\n",
    "            sentence_word_list = []\n",
    "            sentence_word_tag_list = []\n",
    "        else:\n",
    "            sentence_word_list.append(splitted_line[0].strip())\n",
    "            sentence_word_tag_list.append(splitted_line[1].strip())\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import iteritems\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    return np.array([to_ix[w] for w in seq])\n",
    "\n",
    "# Make up some training data\n",
    "training_data = generate_input_data(\"data/train\")\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "ix_to_word = dict([(v, k) for k, v in iteritems(word_to_ix)])\n",
    "\n",
    "char_to_ix = {}\n",
    "for word in word_to_ix.keys():\n",
    "    for char in word:\n",
    "        if char not in char_to_ix:\n",
    "            char_to_ix[char] = len(char_to_ix)\n",
    "ix_to_char = dict([(v, k) for k, v in iteritems(char_to_ix)])\n",
    "\n",
    "tag_to_ix = { \"B\": 0, \"I\": 1, \"O\": 2, LstmCrfTagger.START_TAG: 3, LstmCrfTagger.END_TAG: 4 }\n",
    "ix_to_tag = dict([(v, k) for k, v in iteritems(tag_to_ix)])\n",
    "\n",
    "def prepare_input(sentence, tags):\n",
    "    sent_seq = prepare_sequence(sentence, word_to_ix)\n",
    "    tags_seq = prepare_sequence(tags, tag_to_ix)\n",
    "    word_seq = \\\n",
    "        tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [prepare_sequence(word, char_to_ix) for word in sentence], \n",
    "            padding='post', \n",
    "            value=-1)\n",
    "    word_len_seq = \\\n",
    "        np.apply_along_axis(\n",
    "            lambda seq: next(i for i, j in enumerate(seq) if j < 0), \n",
    "            axis=1, \n",
    "            arr=np.c_[word_seq, np.ones((word_seq.shape[0], 1)) * -1])\n",
    "    return sent_seq, tags_seq, word_seq, np.squeeze(word_len_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "\n",
    "learning_rate = 0.01\n",
    "train_epoch = 10000\n",
    "input_size = 10\n",
    "batch_size = 100\n",
    "num_units = 512\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, None, input_size], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "batch_seq_len = tf.placeholder(tf.int32)\n",
    "org_seq_len = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Bi-LSTM Cell\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw,\n",
    "                                                                     cell_bw=lstm_bw,\n",
    "                                                                     inputs=inputs,\n",
    "                                                                     sequence_length=org_seq_len,\n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# FC\n",
    "W = tf.get_variable(\"W\", [2 * num_units, num_classes], dtype=tf.float32)\n",
    "b = tf.get_variable(\"b\", [num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, batch_seq_len, num_classes])\n",
    "\n",
    "# CRF\n",
    "\n",
    "log_loss, trans_params = tf.contrib.crf.crf_log_likelihood(scores, labels, org_seq_len)\n",
    "loss = tf.reduce_mean(-log_loss)\n",
    "\n",
    "# viterbi Seq, score\n",
    "viterbi_seq, viterbi_score = tf.contrib.crf.crf_decode(scores, trans_params, org_seq_len)\n",
    "\n",
    "# Train Ops\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = train_opt.minimize(loss)\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train into Session\n",
    "\n",
    "# Take From Batch\n",
    "batch_inputs = []\n",
    "batch_labels = []\n",
    "batch_seq_lengths = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_lengths in batch(x_, y_, seq_len_train, batch_size, input_size):\n",
    "            tf_viterbi_seq, _ = sess.run([viterbi_seq, train_op],\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_lengths})\n",
    "        \n",
    "            if i % 30 == 0:\n",
    "                mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "                total_labels = np.sum(batch_seq_lengths)\n",
    "                correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Epoch \", i, \" Accuracy: \", accuracy)\n",
    "        \n",
    "    saver.save(sess, './model_crf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy and Pred\n",
    "\n",
    "# Take From Batch\n",
    "batch_test = []\n",
    "batch_test_labels = []\n",
    "batch_seq_t_lengths = 0\n",
    "\n",
    "with tf.Sesion() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_t_lengths in batch(x_t, y_t, seq_len_test, batch_test_size, input_size):\n",
    "            tf_viterbi_seq = sess.run(viterbi_seq,\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_t_lengths})\n",
    "        \n",
    "            \n",
    "            mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "            total_labels = np.sum(batch_seq_t_lengths)\n",
    "            correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "            accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "            print(\"Test Accuracy: \", accuracy)\n",
    "            print(\"Label: \", batch_labels[0].astype(int))\n",
    "            print(\"Pred: \", tf_viterbi_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EntityExtraction",
   "language": "python",
   "name": "entityextraction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
