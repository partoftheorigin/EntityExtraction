{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import and Pre-Processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['the',\n",
       "   'wall',\n",
       "   'street',\n",
       "   'journal',\n",
       "   'reported',\n",
       "   'today',\n",
       "   'that',\n",
       "   'apple',\n",
       "   'corporation',\n",
       "   'made',\n",
       "   'money'],\n",
       "  ['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']),\n",
       " (['georgia', 'tech', 'is', 'a', 'university', 'in', 'georgia'],\n",
       "  ['B', 'I', 'O', 'O', 'O', 'O', 'B'])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = [ (\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    input_data = list()\n",
    "    sentence_word_list = list()\n",
    "    sentence_word_tag_list = list()\n",
    "    for line in data:\n",
    "        splitted_line = line.split(\"\\t\")\n",
    "\n",
    "        if splitted_line[0] == \"\\n\":\n",
    "            one_sentence_tuple = (sentence_word_list, sentence_word_tag_list)\n",
    "            input_data.append(one_sentence_tuple)\n",
    "            sentence_word_list = []\n",
    "            sentence_word_tag_list = []\n",
    "        else:\n",
    "            sentence_word_list.append(splitted_line[0].strip())\n",
    "            sentence_word_tag_list.append(splitted_line[1].strip())\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-company': 0, 'B-facility': 1, 'B-geo-loc': 2, 'B-movie': 3, 'B-musicartist': 4, 'B-other': 5, 'B-person': 6, 'B-product': 7, 'B-sportsteam': 8, 'B-tvshow': 9, 'I-company': 10, 'I-facility': 11, 'I-geo-loc': 12, 'I-movie': 13, 'I-musicartist': 14, 'I-other': 15, 'I-person': 16, 'I-product': 17, 'I-sportsteam': 18, 'I-tvshow': 19, 'O': 20}\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    return np.array([to_ix[w] for w in seq])\n",
    "\n",
    "# Make up some training data\n",
    "training_data = generate_input_data(\"data/train\")\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "ix_to_word = dict([(v, k) for k, v in iteritems(word_to_ix)])\n",
    "\n",
    "char_to_ix = {}\n",
    "for word in word_to_ix.keys():\n",
    "    for char in word:\n",
    "        if char not in char_to_ix:\n",
    "            char_to_ix[char] = len(char_to_ix)\n",
    "ix_to_char = dict([(v, k) for k, v in iteritems(char_to_ix)])\n",
    "\n",
    "tag_list = ['B-company', 'B-facility', 'B-geo-loc', 'B-movie', 'B-musicartist', 'B-other', 'B-person', 'B-product', 'B-sportsteam',\n",
    " 'B-tvshow', 'I-company', 'I-facility', 'I-geo-loc', 'I-movie', 'I-musicartist', 'I-other', 'I-person', 'I-product', 'I-sportsteam',\n",
    " 'I-tvshow', 'O']\n",
    "\n",
    "tag_to_ix = dict()\n",
    "count = 0\n",
    "for tag in tag_list:\n",
    "    tag_to_ix[tag] = count\n",
    "    count += 1\n",
    "print(tag_to_ix)\n",
    "ix_to_tag = dict([(v, k) for k, v in iteritems(tag_to_ix)])\n",
    "\n",
    "def prepare_input(sentence, tags):\n",
    "    sent_seq = prepare_sequence(sentence, word_to_ix)\n",
    "    tags_seq = prepare_sequence(tags, tag_to_ix)\n",
    "    word_seq = \\\n",
    "        tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [prepare_sequence(word, char_to_ix) for word in sentence], \n",
    "            padding='post', \n",
    "            value=-1)\n",
    "    word_len_seq = \\\n",
    "        np.apply_along_axis(\n",
    "            lambda seq: next(i for i, j in enumerate(seq) if j < 0), \n",
    "            axis=1, \n",
    "            arr=np.c_[word_seq, np.ones((word_seq.shape[0], 1)) * -1])\n",
    "    return sent_seq, tags_seq, word_seq, np.squeeze(word_len_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "\n",
    "learning_rate = 0.01\n",
    "train_epoch = 10000\n",
    "input_size = 10\n",
    "batch_size = 100\n",
    "num_units = 512\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, None, input_size], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "batch_seq_len = tf.placeholder(tf.int32)\n",
    "org_seq_len = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Bi-LSTM Cell\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw,\n",
    "                                                                     cell_bw=lstm_bw,\n",
    "                                                                     inputs=inputs,\n",
    "                                                                     sequence_length=org_seq_len,\n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# FC\n",
    "W = tf.get_variable(\"W\", [2 * num_units, num_classes], dtype=tf.float32)\n",
    "b = tf.get_variable(\"b\", [num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, batch_seq_len, num_classes])\n",
    "\n",
    "# CRF\n",
    "\n",
    "log_loss, trans_params = tf.contrib.crf.crf_log_likelihood(scores, labels, org_seq_len)\n",
    "loss = tf.reduce_mean(-log_loss)\n",
    "\n",
    "# viterbi Seq, score\n",
    "viterbi_seq, viterbi_score = tf.contrib.crf.crf_decode(scores, trans_params, org_seq_len)\n",
    "\n",
    "# Train Ops\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = train_opt.minimize(loss)\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train into Session\n",
    "\n",
    "# Take From Batch\n",
    "batch_inputs = []\n",
    "batch_labels = []\n",
    "batch_seq_lengths = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_lengths in batch(x_, y_, seq_len_train, batch_size, input_size):\n",
    "            tf_viterbi_seq, _ = sess.run([viterbi_seq, train_op],\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_lengths})\n",
    "        \n",
    "            if i % 30 == 0:\n",
    "                mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "                total_labels = np.sum(batch_seq_lengths)\n",
    "                correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Epoch \", i, \" Accuracy: \", accuracy)\n",
    "        \n",
    "    saver.save(sess, './model_crf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy and Pred\n",
    "\n",
    "# Take From Batch\n",
    "batch_test = []\n",
    "batch_test_labels = []\n",
    "batch_seq_t_lengths = 0\n",
    "\n",
    "with tf.Sesion() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_t_lengths in batch(x_t, y_t, seq_len_test, batch_test_size, input_size):\n",
    "            tf_viterbi_seq = sess.run(viterbi_seq,\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_t_lengths})\n",
    "        \n",
    "            \n",
    "            mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "            total_labels = np.sum(batch_seq_t_lengths)\n",
    "            correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "            accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "            print(\"Test Accuracy: \", accuracy)\n",
    "            print(\"Label: \", batch_labels[0].astype(int))\n",
    "            print(\"Pred: \", tf_viterbi_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EntityExtraction",
   "language": "python",
   "name": "entityextraction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
