{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c55146566a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data Import and Pre-Processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Data Import and Pre-Processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_words_raw = pd.read_csv('./data/train', sep='\\t', header=None, names=['word', 'label'])\n",
    "# train_vocab = set(train_words_raw.word.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences_from_words(filename, sep=\"\\t\"):\n",
    "    sequences = []\n",
    "    with open(filename) as fp:\n",
    "        seq = []\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line = line.split(sep)\n",
    "                seq.append(line)\n",
    "            else:\n",
    "                sequences.append(seq)\n",
    "                seq = []\n",
    "        if seq:\n",
    "            sequences.append(seq)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_seq = load_sequences_from_words('./data/train')\n",
    "#train_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    vocab = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            line = line.split('\\t')\n",
    "            vocab.add(line[0])\n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = load_vocab('./data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "\n",
    "learning_rate = 0.01\n",
    "train_epoch = 10000\n",
    "input_size = 10\n",
    "batch_size = 100\n",
    "num_units = 512\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, None, input_size], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "batch_seq_len = tf.placeholder(tf.int32)\n",
    "org_seq_len = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Bi-LSTM Cell\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw,\n",
    "                                                                     cell_bw=lstm_bw,\n",
    "                                                                     inputs=inputs,\n",
    "                                                                     sequence_length=org_seq_len,\n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# FC\n",
    "W = tf.get_variable(\"W\", [2 * num_units, num_classes], dtype=tf.float32)\n",
    "\n",
    "b = tf.get_variable(\"b\", [num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, batch_seq_len, num_classes])\n",
    "\n",
    "# CRF\n",
    "\n",
    "log_loss, trans_params = tf.contrib.crf.crf_log_likelihood(scores, labels, org_seq_len)\n",
    "loss = tf.reduce_mean(-log_loss)\n",
    "\n",
    "# viterbi Seq, score\n",
    "viterbi_seq, viterbi_score = tf.contrib.crf.crf_decode(scores, trans_params, org_seq_len)\n",
    "\n",
    "# Train Ops\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = train_opt.minimize(loss)\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train into Session\n",
    "\n",
    "# Take From Batch\n",
    "batch_inputs = []\n",
    "batch_labels = []\n",
    "batch_seq_lengths = 0\n",
    "\n",
    "with tf.Sesion() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_lengths in batch(x_, y_, seq_len_train, batch_size, input_size):\n",
    "            tf_viterbi_seq, _ = sess.run([viterbi_seq, train_op],\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_lengths})\n",
    "        \n",
    "            if i % 30 == 0:\n",
    "                mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "                total_labels = np.sum(batch_seq_lengths)\n",
    "                correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Epoch \", i, \" Accuracy: \", accuracy)\n",
    "        \n",
    "    saver.save(sess, './model_crf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy and Pred\n",
    "\n",
    "# Take From Batch\n",
    "batch_test = []\n",
    "batch_test_labels = []\n",
    "batch_seq_t_lengths = 0\n",
    "\n",
    "with tf.Sesion() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_t_lengths in batch(x_t, y_t, seq_len_test, batch_test_size, input_size):\n",
    "            tf_viterbi_seq = sess.run(viterbi_seq,\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_t_lengths})\n",
    "        \n",
    "            \n",
    "            mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "            total_labels = np.sum(batch_seq_t_lengths)\n",
    "            correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "            accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "            print(\"Test Accuracy: \", accuracy)\n",
    "            print(\"Label: \", batch_labels[0].astype(int))\n",
    "            print(\"Pred: \", tf_viterbi_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
