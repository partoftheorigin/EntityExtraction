{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import and Pre-Processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_raw = pd.read_csv('./data/train_notypes', sep='\\t', header=None, names=['word', 'label'])\n",
    "train_vocab = set(train_words_raw.word.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import namedtuple\n",
    "class RegexFeatures(object):\n",
    "    PATTERNS = {\n",
    "        \"isInitCapitalWord\": re.compile(r'^[A-Z][a-z]+'),\n",
    "        \"isAllCapitalWord\": re.compile(r'^[A-Z][A-Z]+$'),\n",
    "        \"isAllSmallCase\": re.compile(r'^[a-z]+$'),\n",
    "        \"isWord\": re.compile(r'^[a-zA-Z][a-zA-Z]+$'),\n",
    "        \"isAlphaNumeric\": re.compile(r'^\\p{Alnum}+$'),\n",
    "        \"isSingleCapLetter\": re.compile(r'^[A-Z]$'),\n",
    "        \"containsDashes\": re.compile(r'.*--.*'),\n",
    "        \"containsDash\": re.compile(r'.*\\-.*'),\n",
    "        \"singlePunctuation\": re.compile(r'^\\p{Punct}$'),\n",
    "        \"repeatedPunctuation\": re.compile(r'^[\\.\\,!\\?\"\\':;_\\-]{2,}$'),\n",
    "        \"singleDot\": re.compile(r'[.]'),\n",
    "        \"singleComma\": re.compile(r'[,]'),\n",
    "        \"singleQuote\": re.compile(r'[\\']'),\n",
    "        \"isSpecialCharacter\": re.compile(r'^[#;:\\-/<>\\'\\\"()&]$'),\n",
    "        \"fourDigits\": re.compile(r'^\\d\\d\\d\\d$'),\n",
    "        \"isDigits\": re.compile(r'^\\d+$'),\n",
    "        \"isNumber\": re.compile(r'^((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
    "        \"containsDigit\": re.compile(r'.*\\d+.*'),\n",
    "        \"endsWithDot\": re.compile(r'\\p{Alnum}+\\.$'),\n",
    "        \"isURL\": re.compile(r'^http[s]?://'),\n",
    "        \"isMention\": re.compile(r'^(RT)?@[\\p{Alnum}_]+$'),\n",
    "        \"isHashtag\": re.compile(r'^#\\p{Alnum}+$'),\n",
    "        \"isMoney\": re.compile(r'^\\$((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
    "    }\n",
    "    def __init__(self):\n",
    "        print(\"Initialized RegexFeature\")\n",
    "    @staticmethod\n",
    "    def process(word):\n",
    "        features = dict()\n",
    "        for k, p in RegexFeatures.PATTERNS.iteritems():\n",
    "            if p.match(word):\n",
    "                features[k] = True\n",
    "        return features\n",
    "    \n",
    "    \n",
    "def classification_report_to_df(report):\n",
    "    report_list = []\n",
    "    for i, line in enumerate(report.split(\"\\n\")):\n",
    "        if i == 0:\n",
    "            report_list.append([\"class\", \"precision\", \"recall\", \"f1-score\", \"support\"])\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if line.startswith(\"avg\"):\n",
    "                    line = line.replace(\"avg / total\", \"avg/total\")\n",
    "                line = re.split(r'\\s+', line)\n",
    "                report_list.append(tuple(line))\n",
    "    return pd.DataFrame(report_list[1:], columns=report_list[0])\n",
    "\n",
    "\n",
    "DATA_DIR=\"data/data/\"\n",
    "CLEANED_DIR=\"data/cleaned/\"\n",
    "\n",
    "Tag = namedtuple(\"Tag\", [\"token\", \"tag\"])\n",
    "\n",
    "def load_sequences(filename, sep=\"\\t\", notypes=False, test_data=False):\n",
    "    sequences = []\n",
    "    with open(filename) as fp:\n",
    "        seq = []\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line = line.split(sep)\n",
    "                if test_data:\n",
    "                    assert len(line) == 1\n",
    "                    line.append(\"?\")\n",
    "                if notypes:\n",
    "                    line[1] = line[1][0]\n",
    "                seq.append(Tag(*line))\n",
    "            else:\n",
    "                sequences.append(seq)\n",
    "                seq = []\n",
    "        if seq:\n",
    "            sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def load_vocab(filename):\n",
    "    vocab = set()\n",
    "    with open(filename) as fp:\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            vocab.add(line)\n",
    "    return vocab      \n",
    "\n",
    "    \n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "        \n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr)) \n",
    "        \n",
    "        \n",
    "def plot_cm(y_test, y_pred, labels=[], axis=1):\n",
    "    labels_s = dict((k,i) for i,k in enumerate(labels))\n",
    "    cm = np.zeros((len(labels), len(labels)))\n",
    "    for i,j in zip(sum(y_test, []), sum(y_pred, [])):\n",
    "        i = labels_s[i]\n",
    "        j = labels_s[j]\n",
    "        cm[i,j] += 1\n",
    "    with plt.rc_context(rc={'xtick.labelsize': 12, 'ytick.labelsize': 12,\n",
    "                       'figure.figsize': (16,14)}):\n",
    "        sns.heatmap(cm * 100/ cm.sum(axis=axis, keepdims=True),\n",
    "                    #cmap=sns.cubehelix_palette(n_colors=100, rot=-.4, as_cmap=True),\n",
    "                    cmap=\"Greys\",\n",
    "                    xticklabels=labels,\n",
    "                    yticklabels=labels)\n",
    "        plt.ylabel(\"True labels\")\n",
    "        plt.xlabel(\"Predicted labels\")\n",
    "        title = \"Precision Plot\"\n",
    "        if axis== 0:\n",
    "            title = \"Recall Plot\"\n",
    "        plt.title(title)\n",
    "    print(cm.shape)\n",
    "    return cm\n",
    "\n",
    "\n",
    "def print_sequences(sequences, predictions, filename, test_data=False, notypes=False):\n",
    "    with open(filename, \"wb+\") as fp:\n",
    "        for seq, pred in zip(sequences, predictions):\n",
    "            for t, p in zip(seq, pred):\n",
    "                token, tag = t\n",
    "                if tag[0] == \"U\":\n",
    "                    tag = \"B%s\" % tag[1:]\n",
    "                if tag[0] == \"E\":\n",
    "                    tag = \"I%s\" % tag[1:]\n",
    "                if p[0] == \"U\":\n",
    "                    p = \"B%s\" % p[1:]\n",
    "                if p[0] == \"E\":\n",
    "                    p = \"I%s\" % p[1:]\n",
    "                if notypes:\n",
    "                    tag = tag[0]\n",
    "                    p = p[0]\n",
    "                if test_data:\n",
    "                    line = \"\\t\".join((token, p))\n",
    "                else:\n",
    "                    line = \"\\t\".join((token, tag, p))\n",
    "                print >> fp, line\n",
    "            print >> fp, \"\"\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SPLITTER = re.compile(r'[\\p{Punct}\\s]+')\n",
    "class DictionaryFeatures:\n",
    "    def __init__(self, dictDir):\n",
    "        self.word2dictionaries = {}\n",
    "        self.word2hashtagdictionaries = {}\n",
    "        self.dictionaries = []\n",
    "        i = 0\n",
    "        for d in os.listdir(dictDir):\n",
    "            print >> sys.stderr, \"read dict %s\"%d\n",
    "            self.dictionaries.append(d)\n",
    "            if d == '.svn':\n",
    "                continue\n",
    "            for line in open(dictDir + \"/\" + d):\n",
    "                word = line.rstrip('\\n')\n",
    "                word = word.strip(' ').lower()\n",
    "                word = WORD_SPLITTER.sub(\" \", word)\n",
    "                word_hashtag = \"\".join(WORD_SPLITTER.split(word))\n",
    "                if not self.word2dictionaries.has_key(word):\n",
    "                    self.word2dictionaries[word] = str(i)\n",
    "                else:   \n",
    "                    self.word2dictionaries[word] += \"\\t%s\" % i\n",
    "                if not self.word2hashtagdictionaries.has_key(word_hashtag):\n",
    "                    self.word2hashtagdictionaries[word_hashtag] = str(i)\n",
    "                else:\n",
    "                    self.word2hashtagdictionaries[word_hashtag] += \"\\t%s\" % i\n",
    "            i += 1\n",
    "    \n",
    "    MAX_WINDOW_SIZE=6\n",
    "    def GetDictFeatures(self, words, i):\n",
    "        features = []\n",
    "        phrase = ' '.join(words[i:i+1]).lower().strip(string.punctuation)\n",
    "        phrase = WORD_SPLITTER.sub(\" \", phrase)\n",
    "        if self.word2dictionaries.has_key(phrase):\n",
    "            for j in self.word2dictionaries[phrase].split('\\t'):\n",
    "                features.append('DICT=%s' % self.dictionaries[int(j)])\n",
    "        for window in range(1, self.MAX_WINDOW_SIZE+1):\n",
    "            ## Forward\n",
    "            start=i\n",
    "            end =i + window + 1\n",
    "            if start > -1 and end < len(words) + 1:\n",
    "                phrase = ' '.join(words[start:end]).lower().strip(string.punctuation)\n",
    "                phrase = WORD_SPLITTER.sub(\" \", phrase)\n",
    "                if self.word2dictionaries.has_key(phrase):\n",
    "                    for j in self.word2dictionaries[phrase].split('\\t'):\n",
    "                        features.append('DICTFWD[+%s]=%s' % (window, self.dictionaries[int(j)]))\n",
    "            ## Backward\n",
    "            start = i - window\n",
    "            end =i+1\n",
    "            if start > -1 and end < len(words) + 1:\n",
    "                phrase = ' '.join(words[start:end]).lower().strip(string.punctuation)\n",
    "                phrase = WORD_SPLITTER.sub(\" \", phrase)\n",
    "                if self.word2dictionaries.has_key(phrase):\n",
    "                    for j in self.word2dictionaries[phrase].split('\\t'):\n",
    "                        features.append('DICTBCK[-%s]=%s' % (window, self.dictionaries[int(j)]))\n",
    "            ## Window        \n",
    "            start = i - window\n",
    "            end =i+window+1\n",
    "            if start > -1 and end < len(words) + 1:\n",
    "                phrase = ' '.join(words[start:end]).lower().strip(string.punctuation)\n",
    "                phrase = WORD_SPLITTER.sub(\" \", phrase)\n",
    "                if self.word2dictionaries.has_key(phrase):\n",
    "                    for j in self.word2dictionaries[phrase].split('\\t'):\n",
    "                        features.append('DICTWIN[%s]=%s' % (window, self.dictionaries[int(j)]))\n",
    "                        \n",
    "        \"\"\"\n",
    "        for window in range(1,self.MAX_WINDOW_SIZE):\n",
    "            start=max(i-window+1, 0)\n",
    "            end = start + window\n",
    "            phrase = ' '.join(words[start:end]).lower().strip(string.punctuation)\n",
    "            phrase = WORD_SPLITTER.sub(\" \", phrase)\n",
    "            if self.word2dictionaries.has_key(phrase):\n",
    "                for j in self.word2dictionaries[phrase].split('\\t'):\n",
    "                    features.append('DICT=%s' % self.dictionaries[int(j)])\n",
    "                    if window > 1:\n",
    "                        features.append('DICTWIN[%s]=%s' % (window, self.dictionaries[int(j)]))\n",
    "                        \n",
    "        \"\"\"                \n",
    "        return list(set(features))\n",
    "    \n",
    "    def GetHashtagDictFeatures(self, word):\n",
    "        features = []\n",
    "        if len(word) < 2 or word[0] != \"#\":\n",
    "            return features\n",
    "        word = word[1:].lower().strip(string.punctuation)\n",
    "        if self.word2hashtagdictionaries.has_key(word):\n",
    "            for j in self.word2hashtagdictionaries[word].split('\\t'):\n",
    "                features.append('DICT_HASHTAG=%s' % self.dictionaries[int(j)])\n",
    "        return list(set(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = load_sequences(\"data/train\", sep=\"\\t\", notypes=False)\n",
    "dev_sequences = load_sequences(\"data/dev\", sep=\"\\t\", notypes=False)\n",
    "test_sequences = load_sequences(\"data/test\", sep=\"\\t\", notypes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [[t[0] for t in seq] for seq in (train_sequences+dev_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isHashtag 814\n",
      "isMention 1950\n",
      "isURL 955\n",
      "isMoney 7\n",
      "isNumber 325\n",
      "repeatedPunctuation 1294\n"
     ]
    }
   ],
   "source": [
    "other_entities = {\n",
    "    \"isHashtag\": [],\n",
    "    \"isMention\": [],\n",
    "    \"isURL\": [],\n",
    "    \"isMoney\": [],\n",
    "    \"isNumber\": [],\n",
    "    \"repeatedPunctuation\": []\n",
    "}\n",
    "for seq in all_sentences:\n",
    "    for t in seq:\n",
    "        for k in other_entities.keys():\n",
    "            if RegexFeatures.PATTERNS[k].match(t):\n",
    "                other_entities[k].append(t)\n",
    "for k, v in other_entities.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isHashtag': '__isHashtag__',\n",
       " 'isMention': '__isMention__',\n",
       " 'isURL': '__isURL__',\n",
       " 'isMoney': '__isMoney__',\n",
       " 'isNumber': '__isNumber__',\n",
       " 'repeatedPunctuation': '__repeatedPunctuation__'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENTITY_MAPPINGS={k: \"__%s__\" % k for k in other_entities.keys()}\n",
    "ENTITY_MAPPINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_token(x, to_lower=False):\n",
    "    for k in ENTITY_MAPPINGS.keys():\n",
    "        if RegexFeatures.PATTERNS[k].match(x):\n",
    "            return ENTITY_MAPPINGS[k]\n",
    "    if to_lower:\n",
    "        x = x.lower()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sentences = [[preprocess_token(t[0], to_lower=True) for t in seq] for seq in (train_sequences+dev_sequences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(word2vec_sentences, size=50, window=10, sg=1, hs=0, min_count=1, negative=10, workers=-1, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "\n",
    "learning_rate = 0.01\n",
    "train_epoch = 10000\n",
    "input_size = 10\n",
    "batch_size = 100\n",
    "num_units = 512\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, None, input_size], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "batch_seq_len = tf.placeholder(tf.int32)\n",
    "org_seq_len = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Bi-LSTM Cell\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    \n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw,\n",
    "                                                                     cell_bw=lstm_bw,\n",
    "                                                                     inputs=inputs,\n",
    "                                                                     sequence_length=org_seq_len,\n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "\n",
    "\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# FC\n",
    "W = tf.get_variable(\"W\", [2 * num_units, num_classes], dtype=tf.float32)\n",
    "b = tf.get_variable(\"b\", [num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, batch_seq_len, num_classes])\n",
    "\n",
    "# CRF\n",
    "\n",
    "log_loss, trans_params = tf.contrib.crf.crf_log_likelihood(scores, labels, org_seq_len)\n",
    "loss = tf.reduce_mean(-log_loss)\n",
    "\n",
    "# viterbi Seq, score\n",
    "viterbi_seq, viterbi_score = tf.contrib.crf.crf_decode(scores, trans_params, org_seq_len)\n",
    "\n",
    "# Train Ops\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = train_opt.minimize(loss)\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train into Session\n",
    "\n",
    "# Take From Batch\n",
    "batch_inputs = []\n",
    "batch_labels = []\n",
    "batch_seq_lengths = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_lengths in batch(x_, y_, seq_len_train, batch_size, input_size):\n",
    "            tf_viterbi_seq, _ = sess.run([viterbi_seq, train_op],\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_lengths})\n",
    "        \n",
    "            if i % 30 == 0:\n",
    "                mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "                total_labels = np.sum(batch_seq_lengths)\n",
    "                correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Epoch \", i, \" Accuracy: \", accuracy)\n",
    "        \n",
    "    saver.save(sess, './model_crf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy and Pred\n",
    "\n",
    "# Take From Batch\n",
    "batch_test = []\n",
    "batch_test_labels = []\n",
    "batch_seq_t_lengths = 0\n",
    "\n",
    "with tf.Sesion() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(train_epoch):\n",
    "        for batch_inputs, batch_labels, batch_len, batch_seq_t_lengths in batch(x_t, y_t, seq_len_test, batch_test_size, input_size):\n",
    "            tf_viterbi_seq = sess.run(viterbi_seq,\n",
    "                                        feed_dict={inputs: batch_inputs,\n",
    "                                                  labels: batch_labels,\n",
    "                                                  batch_seq_len:batch_len,\n",
    "                                                  org_seq_len: batch_seq_t_lengths})\n",
    "        \n",
    "            \n",
    "            mask = (np.expand_dims(np.arrange(batch_len), axis=0) < np.expand_dims(batch_seq_lengths, axis=1))\n",
    "            total_labels = np.sum(batch_seq_t_lengths)\n",
    "            correct_labels = np.sum((batch_labels == tf_viterbi_seq)* mask)\n",
    "            accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "            print(\"Test Accuracy: \", accuracy)\n",
    "            print(\"Label: \", batch_labels[0].astype(int))\n",
    "            print(\"Pred: \", tf_viterbi_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EntityExtraction",
   "language": "python",
   "name": "entityextraction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
